{
  "id": "f2b9c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d",
  "timestamp": 1550102400,
  "dateLabel": "14 février 2019",
  "title": "GPT-2 et le débat sur la sécurité - Publication partielle",
  "content": "OpenAI annonce GPT-2, un modèle de 1,5 milliard de paramètres (10x plus grand que GPT-1) et entraîné sur 8 millions de pages web (WebText). Cependant, l'entreprise rompt avec sa politique open source habituelle et ne publie initialement pas le modèle complet, invoquant des risques de \"fake news\" et d'utilisation malveillante (spam, discours de haine). Cette décision déclenche un débat intense dans la communauté AI sur la responsabilité de publication.\n\n**Points clés :**\n- 1,5 milliard de paramètres\n- Publication progressive : modèle réduit (124M) → 774M → complet (1,5B) en novembre 2019\n- Dataset WebText : pages Reddit avec ≥3 upvotes\n- Coût d'entraînement estimé : $256/heure (durée inconnue)",
  "categories": [
    "Technologie",
    "Régulation"
  ],
  "tags": [
    "category:technology",
    "category:regulation",
    "source:official",
    "coverage:mainstream"
  ],
  "source": {
    "name": "OpenAI - Better Language Models",
    "url": "https://openai.com/blog/better-language-models/",
    "reliabilityScore": 0.97,
    "accessedAt": "2026-02-18T09:35:00Z"
  },
  "metadata": {
    "importance": "high",
    "threadId": "main",
    "verificationStatus": "confirmed",
    "crossReferences": [
      "f1a8b2c3-d4e5-4f6a-7b8c-9d0e1f2a3b4c"
    ]
  },
  "publicAwareness": {
    "wasPublicAtTime": true,
    "level": 85,
    "description": "Annonce publique largement couverte par les médias tech et grand public"
  },
  "relevanceScore": 52
}
